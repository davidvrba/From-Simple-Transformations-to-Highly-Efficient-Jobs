{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL I\n",
    "\n",
    "## Task I - Define correct schema for json data\n",
    "\n",
    "* load json dataset\n",
    "* look at the infered schema (is it inferred correctly or is it wrong?)\n",
    "* define the schema explicitly\n",
    "* see what happens if the schema is defined wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, desc, count, explode, split, regexp_replace, collect_list\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('ETL I')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.0\n"
     ]
    }
   ],
   "source": [
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input dataset is in the json format and is in the `data/questios-json` folder. Below is the path definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "\n",
    "project_path = ('/').join(base_path.split('/')[0:-3]) \n",
    "\n",
    "data_input_path = os.path.join(project_path, 'data/questions-json')\n",
    "\n",
    "output_path = os.path.join(project_path, 'output/questions-transformed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First let Spark infer the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionsDF = (\n",
    "    spark\n",
    "    .read\n",
    "    .format('json')\n",
    "    .option('path', data_input_path)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Here we have only 8 json files. In case where you have lots of json files and you know that each file has the same schema, consider loading only one file to check the schema. Inferring the schema from many files can be expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- accepted_answer_id: long (nullable = true)\n",
      " |-- answers: long (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- comments: long (nullable = true)\n",
      " |-- creation_date: timestamp (nullable = true)\n",
      " |-- question_id: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- views: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questionsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the schema:\n",
    "\n",
    "Here it depends on the Spark version. \n",
    "* If Spark version < 3.0, the data type of `creation_date` is inferred as `StringType` however in reality it is a Timestamp. Define the schema by hand and provide it to create the DataFrame\n",
    "* If Spark version >= 3.0, the data type of `creation_date` is inferred correctly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can skip this if using Spark 3.x, because the schema is inferred correctly\n",
    "\n",
    "json_schema = StructType(\n",
    "    [\n",
    "        StructField('question_id', LongType(), True),\n",
    "        StructField('creation_date', TimestampType(), True),\n",
    "        StructField('title', StringType(), True),\n",
    "        StructField('body', StringType(), True),\n",
    "        StructField('tags', StringType(), True),\n",
    "        StructField('accepted_answer_id', LongType(), True),\n",
    "        StructField('answers', LongType(), True),\n",
    "        StructField('comments', LongType(), True),\n",
    "        StructField('user_id', LongType(), True),\n",
    "        StructField('views', LongType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "questionsDF = (\n",
    "    spark\n",
    "    .read\n",
    "    .schema(json_schema)\n",
    "    .format('json')\n",
    "    .option('path', data_input_path)    \n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+----------+----------+------------------+-------+--------+-------+-----+\n",
      "|question_id|creation_date|     title|      body|      tags|accepted_answer_id|answers|comments|user_id|views|\n",
      "+-----------+-------------+----------+----------+----------+------------------+-------+--------+-------+-----+\n",
      "|   61416257|   2020-04...|Ag-Grid...|<p>Ag-g...| <ag-grid>|          61420331|      1|       1|4292512|   21|\n",
      "|   61482176|   2020-04...|Optiona...|<p>My c...|<c#><fu...|              null|      0|       4|3603502|   28|\n",
      "|   61919808|   2020-05...|Matchin...|<p>Ther...|<python...|              null|      0|       3|4453105|   40|\n",
      "|   60340057|   2020-02...|Knockou...|<p>I'm ...|<knocko...|          60340749|      1|       0|3157885|   35|\n",
      "|   62001217|   2020-05...|Python ...|<p>I ru...|<python...|              null|      1|       0|4220475|   17|\n",
      "+-----------+-------------+----------+----------+----------+------------------+-------+--------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questionsDF.show(truncate=10, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195179"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questionsDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happens if the actual data type doesn't match the schema:\n",
    "\n",
    "* set `title` as `LongType` in the defined schema\n",
    "\n",
    "Hint\n",
    "* Different things will happen depending on the `mode` option, where `mode` is one of the following:\n",
    "    * FAILFAST\n",
    "    * DROPMALFORMED\n",
    "    * PERMISSIVE (default)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema with a mistake in the title column:\n",
    "\n",
    "wrong_schema = StructType(\n",
    "    [\n",
    "        StructField('question_id', LongType(), True),\n",
    "        StructField('creation_date', TimestampType(), True),\n",
    "        StructField('title', LongType(), True),\n",
    "        StructField('body', StringType(), True),\n",
    "        StructField('tags', StringType(), True),\n",
    "        StructField('accepted_answer_id', LongType(), True),\n",
    "        StructField('answers', LongType(), True),\n",
    "        StructField('comments', LongType(), True),\n",
    "        StructField('user_id', LongType(), True),\n",
    "        StructField('views', LongType(), True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-----+---------------+---------------+------------------+-------+--------+-------+-----+\n",
      "|question_id|  creation_date|title|           body|           tags|accepted_answer_id|answers|comments|user_id|views|\n",
      "+-----------+---------------+-----+---------------+---------------+------------------+-------+--------+-------+-----+\n",
      "|   61416257|2020-04-24 2...| null|<p>Ag-grid's...|      <ag-grid>|          61420331|      1|       1|4292512|   21|\n",
      "|   61482176|2020-04-28 1...| null|<p>My curren...|<c#><functio...|              null|      0|       4|3603502|   28|\n",
      "|   61919808|2020-05-20 2...| null|<p>There are...|<python><reg...|              null|      0|       3|4453105|   40|\n",
      "|   60340057|2020-02-21 1...| null|<p>I'm hopin...|<knockout.js...|          60340749|      1|       0|3157885|   35|\n",
      "|   62001217|2020-05-25 1...| null|<p>I run a p...|<python><mys...|              null|      1|       0|4220475|   17|\n",
      "+-----------+---------------+-----+---------------+---------------+------------------+-------+--------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    spark\n",
    "    .read\n",
    "    .schema(wrong_schema)\n",
    "    .format('json')\n",
    "    .option('mode', 'PERMISSIVE') # this is the default\n",
    "    .option('path', data_input_path)    \n",
    "    .load()\n",
    ").show(truncate=15, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----+----+----+------------------+-------+--------+-------+-----+\n",
      "|question_id|creation_date|title|body|tags|accepted_answer_id|answers|comments|user_id|views|\n",
      "+-----------+-------------+-----+----+----+------------------+-------+--------+-------+-----+\n",
      "+-----------+-------------+-----+----+----+------------------+-------+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# shows no records\n",
    "(\n",
    "    spark\n",
    "    .read\n",
    "    .schema(wrong_schema)\n",
    "    .format('json')\n",
    "    .option('mode', 'DROPMALFORMED')\n",
    "    .option('path', data_input_path)    \n",
    "    .load()\n",
    ").show(truncate=15, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[question_id: bigint, creation_date: timestamp, title: bigint, body: string, tags: string, accepted_answer_id: bigint, answers: bigint, comments: bigint, user_id: bigint, views: bigint]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# throws an error\n",
    "(\n",
    "    spark\n",
    "    .read\n",
    "    .schema(wrong_schema)\n",
    "    .format('json')\n",
    "    .option('mode', 'FAILFAST')\n",
    "    .option('path', data_input_path)    \n",
    "    .load()\n",
    ")#.show(truncate=15, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "\n",
    "* To read more about schema inferrence and schema evolution of json files in Spark SQL, read my article: https://medium.com/swlh/notes-about-json-schema-handling-in-spark-sql-be1e7f13839d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task II - Transfrom json to parquet and convert String column to an array\n",
    "\n",
    "* convert column tags to array of tags \n",
    "* &lt;tag1&gt;&lt;tag2&gt;&lt;tag3&gt; ---> [tag1, tag2, tag3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert tags to an array\n",
    "\n",
    "Hint\n",
    "* use [split](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.split) to get an array\n",
    "* [explode](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.explode) the array to access each element separately\n",
    "* use [regexp_replace](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.regexp_replace) and split on ><\n",
    "* [groupBy](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy) + [collect_list](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.collect_list)\n",
    "* join with original questions DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultDF = (\n",
    "    questionsDF\n",
    "    .withColumn('tags_arr', split('tags', '><'))\n",
    "    .withColumn('tag', explode('tags_arr'))\n",
    "    .withColumn('tag', regexp_replace('tag', '(<|>)', ''))\n",
    "    .groupBy('question_id')\n",
    "    .agg(collect_list('tag').alias('tags'))\n",
    "    .join(questionsDF.drop('tags'), 'question_id')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "This is an old-school solution used rather before 2.4. Since 2.4 we have higher order functions that can solve the problem more elegantly (we will see that later in the section with Higher Order Functions)\n",
    "\n",
    "There are also some side-effects of this solution:\n",
    "\n",
    "1. groupBy creates a shuffle (quite expensive)\n",
    "2. the elements in the final array may come in different order\n",
    "3. the groupBy key must be unique, otherwise we will reduce it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the data\n",
    "\n",
    "Hint:\n",
    "* use [write](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.write) + [save](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter.save)\n",
    "* [repartition](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.repartition) the data to 8 partitions before saving\n",
    " * this will create 8 files\n",
    " \n",
    "Note\n",
    "* there are also other options how to save data with Spark and we will cover them in the Tables notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    resultDF\n",
    "    .repartition(8)\n",
    "    .write\n",
    "    .mode('overwrite')\n",
    "    .option('path', output_path)\n",
    "    .save()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Check if we saved the data correctly:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkDF = (\n",
    "    spark\n",
    "    .read\n",
    "    .parquet(output_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195179"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------------+----------+----------+------------------+-------+--------+-------+-----+\n",
      "|question_id|      tags|creation_date|     title|      body|accepted_answer_id|answers|comments|user_id|views|\n",
      "+-----------+----------+-------------+----------+----------+------------------+-------+--------+-------+-----+\n",
      "|   44038456|[apache...|   2017-05...|How to ...|<p>I ha...|              null|      8|       1|6214294|20288|\n",
      "|   35059996|[html, ...|   2016-01...|Skip ht...|<p>I ha...|          35061376|      2|       0| 847172|  126|\n",
      "|    6040824|     [wpf]|   2011-05...|Applica...|<p>I wr...|              null|      1|       1| 758659|  102|\n",
      "|   15358973|[ruby-o...|   2013-03...|Rails S...|<p>I am...|              null|      1|       0|1108988| 1051|\n",
      "|   39132750|[androi...|   2016-08...|Android...|<p>I ha...|          39152635|      3|       2|5889375|  632|\n",
      "+-----------+----------+-------------+----------+----------+------------------+-------+--------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkDF.show(truncate=10, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+\n",
      "|tags                                                 |\n",
      "+-----------------------------------------------------+\n",
      "|[apache, reactjs, webpack]                           |\n",
      "|[html, twig, twig-extension]                         |\n",
      "|[wpf]                                                |\n",
      "|[ruby-on-rails-3, http, ssl, savon]                  |\n",
      "|[android, image, background-image, responsive-images]|\n",
      "|[c#, c++, visual-studio-2013, deployment, windows-ce]|\n",
      "|[asp.net-core, odata]                                |\n",
      "|[javascript, google-cloud-firestore]                 |\n",
      "|[android, android-service, battery, batterylevel]    |\n",
      "|[java, swing]                                        |\n",
      "+-----------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkDF.select('tags').show(truncate=False, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- question_id: long (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- creation_date: timestamp (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- accepted_answer_id: long (nullable = true)\n",
      " |-- answers: long (nullable = true)\n",
      " |-- comments: long (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- views: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
