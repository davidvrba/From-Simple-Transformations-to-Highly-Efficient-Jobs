{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "* create cluster of users based on the information about their location\n",
    "* use location coordinates of places from local postgres database\n",
    "* visualise the result of cluster analysis\n",
    "* save the ml model for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, desc, count, explode\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, MinMaxScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('Cluster Analysis I')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "\n",
    "project_path = ('/').join(base_path.split('/')[0:-3]) \n",
    "\n",
    "users_input_path = os.path.join(project_path, 'data/users')\n",
    "\n",
    "model_output_path = os.path.join(project_path, 'output/models/clustering')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get location coordinates\n",
    "\n",
    "We have a table in postgresql database that contains this information. Connect to the database, read the table and create a DataFrame from it. In the next step we will join this information on users.\n",
    "\n",
    "Hint:\n",
    "* see how to connect to jdbc [here](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.jdbc)\n",
    "* you will use format `jdbc`\n",
    "* you will need to provide driver, url, table, user and password in `option` (the values are provided bellow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = 'org.postgresql.Driver'\n",
    "url = 'jdbc:postgresql://localhost/postgres'\n",
    "table = 'public.locations'\n",
    "user = 'postgres'\n",
    "password = 'postgres'\n",
    "\n",
    "\n",
    "locations = (\n",
    "    spark\n",
    "    .read\n",
    "    .format('jdbc')\n",
    "    .option(\"driver\", driver)\n",
    "    .option('url', url)\n",
    "    .option('dbtable', table)\n",
    "    .option('user', user)\n",
    "    .option('password', password)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create DataFrame from users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersDF = (\n",
    "    spark\n",
    "    .read\n",
    "    .option('path', users_input_path)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join users with locations\n",
    "\n",
    "* Create a DataFrame with following columns: `user_id`, `location`, `latitude`, `longitude`\n",
    "* cache the DataFrame, since we will use it in more queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_coord = (\n",
    "    usersDF.alias('users')\n",
    "    .join(locations.alias('locs'), col('users.location') == col('locs.name'))\n",
    "    .select('user_id', 'location', 'latitude', 'longitude')\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_coord.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model\n",
    "\n",
    "* define the array of features (here we have only two freatures: latitude and longitude)\n",
    "* use [VectorAssembler](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler) to create a vector from the features\n",
    "* use [KMeans](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.clustering.KMeans) as the learning algorithm\n",
    "* define the [Pipeline](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.Pipeline) (here it will have only two stages: assembler and kmeans)\n",
    "* train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_array = ['latitude', 'longitude']\n",
    "assembler = VectorAssembler(inputCols=features_array, outputCol='features')\n",
    "kmeans = KMeans(featuresCol='features', predictionCol='predictions', k=6, seed=1)\n",
    "pipeline = Pipeline(stages=[assembler, kmeans])\n",
    "model = pipeline.fit(data_with_coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the model on the data\n",
    "\n",
    "Create a new DataFrame called `predictions`\n",
    "\n",
    "Hint:\n",
    "* the model is a transformer, so you can call transform on it and pass the data\n",
    "* this will add new column `predictions` which contains id of the cluster to which the record belongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(data_with_coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See how big the clusters are\n",
    "\n",
    "Hint:\n",
    "* group by `predictions` and count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions\n",
    "    .groupBy('predictions')\n",
    "    .count()\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See specific cluster\n",
    "\n",
    "Hint:\n",
    "* filter for specific cluster, for example `col('predictions') == 0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions\n",
    "    .select('user_id', 'users.location', *features_array)\n",
    "    .filter(col('predictions') == 0)\n",
    "    .orderBy('location')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions\n",
    "    .select('user_id', 'users.location', *features_array)\n",
    "    .filter(col('predictions') == 1)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions\n",
    "    .select('user_id', 'users.location', *features_array)\n",
    "    .filter(col('predictions') == 2)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions\n",
    "    .select('user_id', 'users.location', *features_array)\n",
    "    .filter(col('predictions') == 3)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions\n",
    "    .select('user_id', 'users.location', *features_array)\n",
    "    .filter(col('predictions') == 4)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions\n",
    "    .select('user_id', 'users.location', *features_array)\n",
    "    .filter(col('predictions') == 5)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise the cluster on the world map\n",
    "\n",
    "Hint:\n",
    "* convert the data with predictios to pandas dataframe\n",
    "* use geopandas library for the plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_clusters = predictions.select('users.location', *features_array, 'predictions').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = geopandas.GeoDataFrame(\n",
    "    local_clusters, \n",
    "    geometry=geopandas.points_from_xy(local_clusters.longitude, local_clusters.latitude)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\n",
    "ax = world.plot(color='white', edgecolor='black')\n",
    "\n",
    "gdf[gdf['predictions'] == 0].plot(ax=ax, color='green')\n",
    "gdf[gdf['predictions'] == 1].plot(ax=ax, color='blue')\n",
    "gdf[gdf['predictions'] == 2].plot(ax=ax, color='red')\n",
    "gdf[gdf['predictions'] == 3].plot(ax=ax, color='black')\n",
    "gdf[gdf['predictions'] == 4].plot(ax=ax, color='yellow')\n",
    "gdf[gdf['predictions'] == 5].plot(ax=ax, color='violet')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the result\n",
    "\n",
    "Save the DataFrame with predictions to a table in postgresql databaze\n",
    "\n",
    "Hint:\n",
    "* use the `format` jdbc\n",
    "* use append `mode`\n",
    "* provide url, table_name, user, password in `option`\n",
    "    * the url, user and password are the same as we used for reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result to the database:\n",
    "\n",
    "table_name = 'user_clusters'\n",
    "\n",
    "(\n",
    "    predictions\n",
    "    .select(\n",
    "        'user_id', \n",
    "        col('predictions').alias('cluster_id')\n",
    "    )\n",
    "    .write\n",
    "    .mode('append')\n",
    "    .format('jdbc')\n",
    "    .option('url', url)\n",
    "    .option('dbtable', table_name)\n",
    "    .option('user', user)\n",
    "    .option('password', password)\n",
    "    .save()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model on disk\n",
    "\n",
    "Persisting the model allows you to load it in your production application and use it on new data\n",
    "\n",
    "Hint:\n",
    "* use [write](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.clustering.KMeans.write) to create [MLWriter](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.util.MLWriter)\n",
    "* then use [save](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.clustering.KMeans.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    model\n",
    "    .write()\n",
    "    .overwrite()\n",
    "    .save(model_output_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model again\n",
    "\n",
    "Load the model from the path to test that it works.\n",
    "\n",
    "* use API of [PipelineModel](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.PipelineModel)\n",
    "    * [read](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.PipelineModel.read)\n",
    "* apply the loaded model on our data and group by `predictions` to see it gives the same result as the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = (\n",
    "    PipelineModel\n",
    "    .read()\n",
    "    .load(model_output_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.transform(data_with_coord).groupBy('predictions').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
