{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Analytics\n",
    "\n",
    "## Task I\n",
    "\n",
    "* build ML prototype that will predict if a question will be ansered in the next 2 hours\n",
    "* model it as binary classification\n",
    "* first prepare simple model with some basic features\n",
    "* then try to improve it by adding some more features\n",
    "* use random forest as a classifier\n",
    "* for modelling consider only questions that have accepted answer\n",
    "* if you run in local mode do not hyperparameter tuning since it may run to long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, unix_timestamp, when, lit, length, array_sort, udf, desc\n",
    ")\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    ArrayType, StructType, StructField, StringType, IntegerType\n",
    ")\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, CrossValidatorModel, ParamGridBuilder\n",
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Tokenizer, SQLTransformer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('Predictive Analytics I')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "\n",
    "project_path = ('/').join(base_path.split('/')[0:-3]) \n",
    "\n",
    "answers_input_path = os.path.join(project_path, 'data/answers')\n",
    "\n",
    "questions_input_path = os.path.join(project_path, 'output/questions-transformed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Load the data:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answersDF = (\n",
    "    spark\n",
    "    .read\n",
    "    .option('path', answers_input_path)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "questionsDF = (\n",
    "    spark\n",
    "    .read\n",
    "    .option('path', questions_input_path)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Add label to the dataset</b>\n",
    "\n",
    "hint:\n",
    "* join questions with answers\n",
    "* compute response time using unix_timestamp\n",
    "* use 'when' condition to compute the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_label = (\n",
    "    questionsDF.alias('questions')\n",
    "    .join(answersDF.alias('answers'), questionsDF['accepted_answer_id'] == answersDF['answer_id'])\n",
    "    .select(\n",
    "        col('questions.tags'),\n",
    "        col('questions.creation_date').alias('question_time'),\n",
    "        col('questions.title'),\n",
    "        col('questions.body').alias('message'),\n",
    "        col('answers.creation_date').alias('answer_time')\n",
    "    )\n",
    "    .withColumn('response_time', unix_timestamp('answer_time') - unix_timestamp('question_time'))\n",
    "    .withColumn('label', when(col('response_time') <= 7200, lit(1)).otherwise(0))\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_label.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Take a look at the distribution of classes</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    data_with_label\n",
    "    .groupBy('label')\n",
    "    .count()\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Add some basic features:</b>\n",
    "\n",
    "hint:\n",
    "* add feature 'title_complexity'\n",
    " * compute the length of the question title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_basic_features = (\n",
    "    data_with_label\n",
    "    .withColumn('title_complexity', length('title'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Prepare data</b>\n",
    "\n",
    "hint:\n",
    "* split the data for training and testing using randomSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data_with_basic_features.randomSplit([0.7, 0.3], 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Build the pipeline and train the model:</b>\n",
    "\n",
    "hint:\n",
    "* use: \n",
    " * VectorAssembler\n",
    " * RandomForestClassifier\n",
    " * Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['title_complexity']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=(features), outputCol='features')\n",
    "\n",
    "# Classifier:\n",
    "rf = RandomForestClassifier(labelCol='label', featuresCol='features', seed=42)\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "rf_model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Evaluate the model</b>\n",
    "\n",
    "hint:\n",
    "* use BinaryClassificationEvaluator with areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol='label', metricName='areaUnderROC')\n",
    "\n",
    "predictions = rf_model.transform(test_data)\n",
    "\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Add more features</b>\n",
    "\n",
    "hint:\n",
    "* add features: \n",
    "    * 'question_size' number of words in the question body\n",
    "    * use Tokenizer to split the text on words\n",
    "    * use a SQLTransformer to compute the size\n",
    "    \n",
    "* train the model with this new pipeline\n",
    "* evaluate the model\n",
    "* see if the model improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeTrans = SQLTransformer(statement=\"SELECT *, size(words) AS message_size FROM __THIS__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['title_complexity', 'message_size']\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='message', outputCol='words')\n",
    "\n",
    "assembler = VectorAssembler(inputCols=(features), outputCol='features')\n",
    "\n",
    "rf = RandomForestClassifier(labelCol='label', featuresCol='features', seed=42)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, sizeTrans, assembler, rf])\n",
    "\n",
    "rf_model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol='label', metricName='areaUnderROC')\n",
    "\n",
    "predictions = rf_model.transform(test_data)\n",
    "\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "\n",
    "* Similarly you could look for other features and try to improve the evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Hyperparameter tuning:</b>\n",
    "\n",
    "hint:\n",
    "* use ParamGridBuilder to find optimal numTrees and optimal masDepth\n",
    "\n",
    "Note:\n",
    "\n",
    "If you run in local mode skip the hyperparameter tuning since it may run to long (more then hour)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = (\n",
    "  ParamGridBuilder()\n",
    "  .addGrid(rf.maxDepth, [3, 5, 8])\n",
    "  .addGrid(rf.numTrees, [50, 100, 150])\n",
    "  .build()\n",
    ")\n",
    "\n",
    "#cross_model = CrossValidator(estimator=pipeline, evaluator=evaluator, estimatorParamMaps=paramGrid).fit(train_data)\n",
    "#rf_model = cross_model.bestModel\n",
    "#predictions = rf_model.transform(test_data)\n",
    "#evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
