{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Based Optimization\n",
    "\n",
    "## Task: see how statistics are used\n",
    "\n",
    "* turn CBO on\n",
    "* save dataframe as table using metastore\n",
    "* run simple query and see the query plan with stats using EXPLAIN COST\n",
    "    * since Spark 3.0 we can use `explain(mode='cost')`\n",
    "* run ANALYZE TABLE and see it again\n",
    "* compute stats for individual cols and see the difference\n",
    "* compute the histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('CBO')\n",
    "    .config(\"spark.sql.hive.metastore.version\", \"1.2.1\")\n",
    "    .config(\"spark.sql.hive.metastore.jars\", \"maven\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turn CBO on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.cbo.enabled', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "\n",
    "project_path = ('/').join(base_path.split('/')[0:-3]) \n",
    "\n",
    "users_input_path = os.path.join(project_path, 'data/users')\n",
    "\n",
    "users_output_path = os.path.join(project_path, 'output/users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersDF = spark.read.load(users_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See the query plan with stats\n",
    "\n",
    "Hint:\n",
    "* compose a query with filter user_id < -1000 (we know that there are no such records)\n",
    "* use explain with mode='cost' to see the plan with stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(user_id#0L) AND (user_id#0L < -1000)), Statistics(sizeInBytes=4.1 MiB)\n",
      "+- Relation[user_id#0L,display_name#1,about#2,location#3,downvotes#4L,upvotes#5L,reputation#6L,views#7L] parquet, Statistics(sizeInBytes=4.1 MiB)\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [user_id#0L, display_name#1, about#2, location#3, downvotes#4L, upvotes#5L, reputation#6L, views#7L]\n",
      "+- *(1) Filter (isnotnull(user_id#0L) AND (user_id#0L < -1000))\n",
      "   +- *(1) ColumnarToRow\n",
      "      +- FileScan parquet [user_id#0L,display_name#1,about#2,location#3,downvotes#4L,upvotes#5L,reputation#6L,views#7L] Batched: true, DataFilters: [isnotnull(user_id#0L), (user_id#0L < -1000)], Format: Parquet, Location: InMemoryFileIndex[file:/Users/david.vrba/spark-trainings/From-Simple-Transformations-to-Highly-Ef..., PartitionFilters: [], PushedFilters: [IsNotNull(user_id), LessThan(user_id,-1000)], ReadSchema: struct<user_id:bigint,display_name:string,about:string,location:string,downvotes:bigint,upvotes:b...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    usersDF\n",
    "    .filter(col('user_id') < -1000)\n",
    ").explain(mode='cost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now save the table using metastore\n",
    "\n",
    "Hint\n",
    "* use `saveAsTable()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    usersDF\n",
    "    .write\n",
    "    .mode('overwrite')\n",
    "    .option('path', users_output_path)\n",
    "    .saveAsTable('users')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n",
      "|database|           tableName|isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "| default|         data_summit|      false|\n",
      "| default|data_summit_parti...|      false|\n",
      "| default|      delta_table_v1|      false|\n",
      "| default|      delta_table_v2|      false|\n",
      "| default|     insert_table_v3|      false|\n",
      "| default|           questions|      false|\n",
      "| default|        questions_ss|      false|\n",
      "| default|questions_ss_part...|      false|\n",
      "| default|questions_ss_part...|      false|\n",
      "| default|  spark_summit_users|      false|\n",
      "| default|          test_table|      false|\n",
      "| default|               users|      false|\n",
      "+--------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see the tables we have:\n",
    "\n",
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See the query plan again\n",
    "\n",
    "* Now read the table from metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(user_id#72L) AND (user_id#72L < -1000)), Statistics(sizeInBytes=4.1 MiB)\n",
      "+- Relation[user_id#72L,display_name#73,about#74,location#75,downvotes#76L,upvotes#77L,reputation#78L,views#79L] parquet, Statistics(sizeInBytes=4.1 MiB)\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [user_id#72L, display_name#73, about#74, location#75, downvotes#76L, upvotes#77L, reputation#78L, views#79L]\n",
      "+- *(1) Filter (isnotnull(user_id#72L) AND (user_id#72L < -1000))\n",
      "   +- *(1) ColumnarToRow\n",
      "      +- FileScan parquet default.users[user_id#72L,display_name#73,about#74,location#75,downvotes#76L,upvotes#77L,reputation#78L,views#79L] Batched: true, DataFilters: [isnotnull(user_id#72L), (user_id#72L < -1000)], Format: Parquet, Location: InMemoryFileIndex[file:/Users/david.vrba/spark-trainings/From-Simple-Transformations-to-Highly-Ef..., PartitionFilters: [], PushedFilters: [IsNotNull(user_id), LessThan(user_id,-1000)], ReadSchema: struct<user_id:bigint,display_name:string,about:string,location:string,downvotes:bigint,upvotes:b...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    spark.table('users')\n",
    "    .filter(col('user_id') < -1000)\n",
    ").explain(mode='cost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See the statistics for the table\n",
    "\n",
    "Hint\n",
    "* use sql DESC EXTENDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------------------------------------------+-------+\n",
      "|                    col_name|                                                   data_type|comment|\n",
      "+----------------------------+------------------------------------------------------------+-------+\n",
      "|                     user_id|                                                      bigint|   null|\n",
      "|                display_name|                                                      string|   null|\n",
      "|                       about|                                                      string|   null|\n",
      "|                    location|                                                      string|   null|\n",
      "|                   downvotes|                                                      bigint|   null|\n",
      "|                     upvotes|                                                      bigint|   null|\n",
      "|                  reputation|                                                      bigint|   null|\n",
      "|                       views|                                                      bigint|   null|\n",
      "|                            |                                                            |       |\n",
      "|# Detailed Table Information|                                                            |       |\n",
      "|                    Database|                                                     default|       |\n",
      "|                       Table|                                                       users|       |\n",
      "|                       Owner|                                                  david.vrba|       |\n",
      "|                Created Time|                               Sat Oct 17 13:45:33 CEST 2020|       |\n",
      "|                 Last Access|                                                     UNKNOWN|       |\n",
      "|                  Created By|                                                 Spark 3.0.0|       |\n",
      "|                        Type|                                                    EXTERNAL|       |\n",
      "|                    Provider|                                                     parquet|       |\n",
      "|                    Location|file:/Users/david.vrba/spark-trainings/From-Simple-Transf...|       |\n",
      "|               Serde Library| org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe|       |\n",
      "+----------------------------+------------------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('DESC EXTENDED users').show(truncate=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the statistics\n",
    "\n",
    "Hint\n",
    "* run sql ANALYZE TABLE ... COMPUTE STATISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('ANALYZE TABLE users COMPUTE STATISTICS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See the stats again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------------------------------------------+-------+\n",
      "|                    col_name|                                                   data_type|comment|\n",
      "+----------------------------+------------------------------------------------------------+-------+\n",
      "|                     user_id|                                                      bigint|   null|\n",
      "|                display_name|                                                      string|   null|\n",
      "|                       about|                                                      string|   null|\n",
      "|                    location|                                                      string|   null|\n",
      "|                   downvotes|                                                      bigint|   null|\n",
      "|                     upvotes|                                                      bigint|   null|\n",
      "|                  reputation|                                                      bigint|   null|\n",
      "|                       views|                                                      bigint|   null|\n",
      "|                            |                                                            |       |\n",
      "|# Detailed Table Information|                                                            |       |\n",
      "|                    Database|                                                     default|       |\n",
      "|                       Table|                                                       users|       |\n",
      "|                       Owner|                                                  david.vrba|       |\n",
      "|                Created Time|                               Sat Oct 17 13:45:33 CEST 2020|       |\n",
      "|                 Last Access|                                                     UNKNOWN|       |\n",
      "|                  Created By|                                                 Spark 3.0.0|       |\n",
      "|                        Type|                                                    EXTERNAL|       |\n",
      "|                    Provider|                                                     parquet|       |\n",
      "|                  Statistics|                                  4292212 bytes, 124225 rows|       |\n",
      "|                    Location|file:/Users/david.vrba/spark-trainings/From-Simple-Transf...|       |\n",
      "+----------------------------+------------------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('DESC EXTENDED users').show(truncate=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See the query plan for the query again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('EXPLAIN COST select * from users where user_id < -1000').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See column level stats\n",
    "\n",
    "Hint\n",
    "* use DESC EXTENDED table_name, col_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('DESC EXTENDED users user_id').show(truncate=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute column level stats\n",
    "\n",
    "Hint:\n",
    "* use ANALYZE TABLE table_name COMPUTE STATISTICS FOR COLUMNS col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('ANALYZE TABLE users COMPUTE STATISTICS FOR COLUMNS user_id, display_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See the stats again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('DESC EXTENDED users user_id').show(truncate=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See the plan again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('EXPLAIN COST select * from users where user_id < -1000').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the histogram for specific cols\n",
    "\n",
    "Hint\n",
    "* Check if histogram is enabled\n",
    "* Enable if not\n",
    "* Compute column level stats again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get('spark.sql.statistics.histogram.enabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.statistics.histogram.enabled', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('ANALYZE TABLE users COMPUTE STATISTICS FOR COLUMNS user_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See the stats again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('DESC EXTENDED users user_id').show(truncate=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
