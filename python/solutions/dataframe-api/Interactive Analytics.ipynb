{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Analytics\n",
    "\n",
    "In this notebook you will answer 2 basic analytical questions about the data and visualise the result using Python library Matplotlib. We will see one way how Spark is integrated with Python library Pandas which allows you to access also other libraries of the Python ecosystem, for example Matplotlib for visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, window, count, unix_timestamp, when, lit, ceil\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('Interactive Analytics I')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "\n",
    "project_path = ('/').join(base_path.split('/')[0:-3]) \n",
    "\n",
    "answers_input_path = os.path.join(project_path, 'data/answers')\n",
    "\n",
    "questions_input_path = os.path.join(project_path, 'output/questions-transformed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task I\n",
    "\n",
    "* Find out how many answers are being produced per week\n",
    "* Plot the time evolution: on the x axis have date dimmension, on the y axis have number of answers per week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answersDF = (\n",
    "    spark\n",
    "    .read\n",
    "    .option('path', answers_input_path)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answersDF.show(truncate=25, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group the data\n",
    "\n",
    "Hint:\n",
    "* use `groupBy(window)`, where the `window` will be \"1 week\"\n",
    "* docs for [window](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.window.html#pyspark.sql.functions.window)\n",
    "* the output of grouping by `window` will be struct with two subfields `start` and `end`\n",
    "* use the `start` subfield and change the type to `date` - this will be used in the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedDF = (\n",
    "    answersDF\n",
    "    .filter(col('user_id').isNotNull())\n",
    "    .groupBy(\n",
    "        window('creation_date', \"1 week\")\n",
    "    )\n",
    "    .agg(\n",
    "        count('*').alias('answers')\n",
    "    )\n",
    "    .withColumn('date', col('window.start').cast('date'))\n",
    "    .orderBy('window')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedDF.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise the data:\n",
    "\n",
    "Hint\n",
    "* convert the aggregated data to Pandas dataframe using [toPandas()](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.toPandas.html#pyspark.sql.DataFrame.toPandas)\n",
    "* use ploting options of Pandas dataframe\n",
    " * [plot](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data = groupedDF.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data.plot(\n",
    "    x='date', y='answers', figsize=(12, 6), \n",
    "    title='Number of answers per week',\n",
    "    legend=False,\n",
    "    xlabel='Date',\n",
    "    ylabel='Number of answers',\n",
    "    kind='line'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task II\n",
    "\n",
    "* Compute the response time\n",
    " * for each question compute the time it took to have accepted answer\n",
    " * consider only questions with accepted answer\n",
    "* Plot number of answered questions as a function of response time\n",
    " * choose hour as the time unit\n",
    " * create a bar chart (too see how many questions were answered within first hour, within second hour and so on)\n",
    " * plot a cumulative sum (too see for example how many questions in total were answered within first 10 hours and so on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionsDF = (\n",
    "    spark\n",
    "    .read\n",
    "    .option('path', questions_input_path)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute response time:\n",
    "\n",
    "For each question compute how long it took to get accepted answer. Consider only questions that actually have accepted answers.\n",
    "\n",
    "Hint:\n",
    "* for each question and answer we now the time when it was created (`created_date`)\n",
    "* [join](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.join.html#pyspark.sql.DataFrame.join) questions with answers (use `accepted_answer_id` field in the join)\n",
    "* use [unix_timestamp](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.unix_timestamp.html#pyspark.sql.functions.unix_timestamp) to compare the times\n",
    "* convert to hours\n",
    "* [ceil](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.ceil.html#pyspark.sql.functions.ceil) the numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_data = (\n",
    "    questionsDF.alias('questions')\n",
    "    .join(answersDF.alias('answers'), questionsDF['accepted_answer_id'] == answersDF['answer_id'])\n",
    "    .select(\n",
    "        col('questions.creation_date').alias('question_time'),\n",
    "        col('answers.creation_date').alias('answer_time')\n",
    "    )\n",
    "    .withColumn('response_time', unix_timestamp('answer_time') - unix_timestamp('question_time'))\n",
    "    .filter(col('response_time') > 0)\n",
    "    .withColumn('hours', ceil(col('response_time') / 3600))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate the data and visualise:\n",
    "\n",
    "Hint:\n",
    "* group by hour\n",
    "* count\n",
    "* convert to Pandas\n",
    "* visualize (take first 24 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_data_grouped = (\n",
    "    hourly_data\n",
    "    .groupBy('hours')\n",
    "    .agg(count('*').alias('cnt'))\n",
    "    .orderBy('hours')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_data_local = hourly_data_grouped.limit(24).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the data localy:\n",
    "\n",
    "hourly_data_local.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For bar chart you can use df.plot.bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_data_local.plot(\n",
    "    x='hours', y='cnt', figsize=(12, 6), \n",
    "    title='Response time of questions',\n",
    "    legend=False,\n",
    "    kind='bar',\n",
    "    xlabel='Hour',\n",
    "    ylabel='Number of answered questions'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "\n",
    "As you can see, big portion of the questions that have accepted answer were answered within the first hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cumulative sum\n",
    "\n",
    "* To compute cumulative sum you can use [cumsum()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.cumsum.html)\n",
    "* add new col to the Pandas DataFrame as df['new_col'] = df['cnt'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hourly_data_local['cumsum'] = hourly_data_local['cnt'].cumsum()\n",
    "\n",
    "hourly_data_local.plot(\n",
    "    x='hours',\n",
    "    y='cumsum',\n",
    "    figsize=(12, 6),\n",
    "    title='Cumulative size of answered questions',\n",
    "    xlabel='Hour',\n",
    "    ylabel='Number of answered questions',\n",
    "    legend=False\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also see what is the total number of questions with accepted answer:\n",
    "\n",
    "(\n",
    "    questionsDF.alias('questions')\n",
    "    .join(answersDF.alias('answers'), questionsDF['accepted_answer_id'] == answersDF['answer_id'])\n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
