{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task - simple text categorization\n",
    "\n",
    "* In this notebook we will see how to dynamically process lots of culumns using a function as wrapper over native functions\n",
    "* Assume we have some categories (labels or tags) and we want to do simple text categorization. For each of these categories find out if the category is contained in the body of the question\n",
    "* Implement a function that will add a new column for each of these categories. The name of the column should be the name of the category and the value is 1/0 depending on wether the word is contained in the text.\n",
    "* According to this rule each question can belong to multiple groups\n",
    "* As the final result, sum for each of these categories, to how many questions in belong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, when, sum\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('Text search')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "\n",
    "project_path = ('/').join(base_path.split('/')[0:-3]) \n",
    "\n",
    "questions_input_path = os.path.join(project_path, 'output/questions-transformed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionsDF = (\n",
    "    spark\n",
    "    .read\n",
    "    .option('path', questions_input_path)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First do it without a function\n",
    "\n",
    "Hint:\n",
    "* use `when` - `otherwise` condition together with the `like` function\n",
    " * `like` in docs: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.like\n",
    " * `when` in docs https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.when\n",
    "* at the end, sum the occurences using df.select(sum(..), sum(..), ...)\n",
    "* the output should be a DataFrame with one row\n",
    "\n",
    "* Here are the categories: `['java', 'sql', 'python', 'spark']`\n",
    "* Here are the corresponding column names: `['java', 'sql', 'python', 'spark']` (they are the same as the categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement a function\n",
    "\n",
    "* Now do the same using a function\n",
    "* The function should take a DataFrame as input and return a new DataFrame with new columns\n",
    "* The function will simply add for each category new col using some `for-loop` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['java', 'sql', 'python', 'spark']\n",
    "\n",
    "# your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final sum\n",
    "\n",
    "* Use Python `map + lambda` function to iterate over the column names and apply the `sum` with `alias` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
