{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL I\n",
    "\n",
    "## Task I - Define correct schema for json data\n",
    "\n",
    "* load json dataset\n",
    "* look at the infered schema (is it inferred correctly or is it wrong?)\n",
    "* define the schema explicitly\n",
    "* see what happens if the schema is defined wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, desc, count, explode, split, regexp_replace, collect_list\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('ETL I')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input dataset is in the json format and is in the `data/questions-json` folder. Below is the path definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "\n",
    "project_path = ('/').join(base_path.split('/')[0:-3]) \n",
    "\n",
    "data_input_path = os.path.join(project_path, 'data/questions-json')\n",
    "\n",
    "output_path = os.path.join(project_path, 'output/questions-transformed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First let Spark infer the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Here we have only 8 json files. In case where you have lots of json files and you know that each file has the same schema, consider loading only one file to check the schema. Inferring the schema from many files can be expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the schema:\n",
    "\n",
    "Here it depends on the Spark version. \n",
    "* If Spark version is different from 3.0, the data type of `creation_date` is inferred as `StringType` however in reality it is a Timestamp. Define the schema by hand and provide it to create the DataFrame\n",
    "* If Spark version = 3.0, the data type of `creation_date` is inferred correctly as `TimestampType`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here:\n",
    "\n",
    "# You can skip this if using Spark 3.x, because the schema is inferred correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happens if the actual data type doesn't match the schema:\n",
    "\n",
    "* set `title` as `LongType` in the defined schema\n",
    "\n",
    "Hint\n",
    "* Different things will happen depending on the `mode` option, where `mode` is one of the following:\n",
    "    * FAILFAST\n",
    "    * DROPMALFORMED\n",
    "    * PERMISSIVE (default)\n",
    "* For more details about the mode and also other json options see the [docs](https://spark.apache.org/docs/latest/sql-data-sources-json.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema with a mistake in the title column:\n",
    "# your code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the default PERMISSIVE mode\n",
    "# your code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the DROPMALFORMED mode\n",
    "# your code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the FAILFAST mode\n",
    "# your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "\n",
    "* To read more about schema inferrence and schema evolution of json files in Spark SQL, read my article: https://medium.com/swlh/notes-about-json-schema-handling-in-spark-sql-be1e7f13839d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task II - Transfrom json to parquet and convert String column to an array\n",
    "\n",
    "* convert column tags to array of tags \n",
    "* &lt;tag1&gt;&lt;tag2&gt;&lt;tag3&gt; ---> [tag1, tag2, tag3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert tags to an array\n",
    "\n",
    "Hint\n",
    "* use [split](http://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.split.html#pyspark.sql.functions.split) to get an array\n",
    "* [explode](http://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.explode.html#pyspark.sql.functions.explode) the array to access each element separately\n",
    "* use [regexp_replace](http://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.regexp_replace.html#pyspark.sql.functions.regexp_replace) and split on ><\n",
    "* [groupBy](http://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.groupBy.html#pyspark.sql.DataFrame.groupBy) + [collect_list](http://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.collect_list.html#pyspark.sql.functions.collect_list)\n",
    "* join with original questions DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "This is an old-school solution used rather before 2.4. Since 2.4 we have higher order functions that can solve the problem more elegantly (we will see that later in the section with Higher Order Functions)\n",
    "\n",
    "There are also some side-effects of this solution:\n",
    "\n",
    "1. groupBy creates a shuffle (quite expensive)\n",
    "2. the elements in the final array may come in different order\n",
    "3. the groupBy key must be unique, otherwise we will reduce it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the data\n",
    "\n",
    "Hint:\n",
    "* use [write](http://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.write.html#pyspark.sql.DataFrame.write) + [save](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.save.html#pyspark.sql.DataFrameWriter.save)\n",
    "* [repartition](http://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.repartition.html#pyspark.sql.DataFrame.repartition) the data to 8 partitions before saving\n",
    " * this will create 8 files\n",
    " \n",
    "Note\n",
    "* there are also other options how to save data with Spark and we will cover them in the Tables notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if we saved the data correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
