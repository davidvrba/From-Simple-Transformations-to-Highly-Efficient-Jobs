{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucketing & Partitioning\n",
    "\n",
    "In this notebook you will see the advantages of partitioning and bucketing. This notebook depends on the result of the two previous notebooks (so run them first):\n",
    "* Partitioning I\n",
    "* Bucketing I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, desc\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('Bucketing II')\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task I\n",
    "\n",
    "* join users with questions\n",
    " * take questions only for the year 2018\n",
    " * see the query plan\n",
    "* turn off broadcast hash join to see the consequence of bucketing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "\n",
    "project_path = ('/').join(base_path.split('/')[0:-2]) \n",
    "\n",
    "questions_input_path = os.path.join(project_path, 'output/1/questions-partitioned')\n",
    "\n",
    "users_input_path = os.path.join(project_path, 'output/users-bucketed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "\n",
    "If you are using derby database as the metastore (which is the default setting in local mode) and you connected to the metastore in some other notebook already, the next command will fail since you can connect to the database only from one application. In this case shut down your jupyter notebook and start it again (restarting the kernel is not enough)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the data:\n",
    "\n",
    "Hint:\n",
    "* read users from the table\n",
    " * use spark.table(table_name)\n",
    "* read questions from the partitioned layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersDF = spark.table('users')\n",
    "\n",
    "questionsDF = (\n",
    "    spark\n",
    "    .read\n",
    "    .option('path', questions_input_path)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turn off broadcast hash join:\n",
    "\n",
    "Hint:\n",
    "* set autoBroadcastJoinThreshold to -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.autoBroadcastJoinThreshold', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the join query:\n",
    "\n",
    "Hint:\n",
    "* use repartition(10, 'user_id') to achieve 'one-side shuffle-free join'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    questionsDF\n",
    "    .filter(col('year').isin([2018, 2017, 2016]))\n",
    "    .repartition(10, 'user_id')\n",
    "    .join(usersDF, 'user_id')\n",
    "    .select('user_id', 'year')\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See the query plan\n",
    "\n",
    "Hint:\n",
    "* go to Spark UI to the sql tab\n",
    "* from the plan you should see:\n",
    " * partition pruning\n",
    " * Exchange only in one branch of the plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task II\n",
    "\n",
    "Do the same as before but this time filter for specific user = 8440. Go to see the query plan. You should see in Parquet Scan node that only 1 bucket was selected and that it scans much less data (See the input size to the first stage of the job).\n",
    "\n",
    "Hint:\n",
    "* use collect instead of show to see the real difference in data size that is scaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    questionsDF\n",
    "    .filter(col('year').isin([2018, 2017, 2016]))\n",
    "    .repartition(10, 'user_id')\n",
    "    .join(usersDF, 'user_id')\n",
    "    .filter(col('user_id') == 8440)\n",
    "    .select('user_id', 'year')\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
