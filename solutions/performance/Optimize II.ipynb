{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize the query plan II\n",
    "\n",
    "Suppose we want to join badges with users (using the table from the metastore). We also want to use a UDF (which does some computation on the badges.name field) and using a window we want for each user order the badges depending on the creation date. \n",
    "\n",
    "See the query bellow which does that in suboptimal way and try to rewrite it to achieve more optimal plan. More specifically try to eliminate the Exchange in the query plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, udf, row_number\n",
    ")\n",
    "\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('Optimize II')\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "\n",
    "project_path = ('/').join(base_path.split('/')[0:-2]) \n",
    "\n",
    "badges_input_path = os.path.join(project_path, 'data/badges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersDF = spark.table('users')\n",
    "\n",
    "badgesDF = (\n",
    "    spark\n",
    "    .read\n",
    "    .option('path', badges_input_path)\n",
    "    .load()    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UDF:\n",
    "\n",
    "The UDF bellow is just simple function that gets the lenght of a string. This can be easily done using native pyspark dataframe function length. For the sake of this example however suppose that this function encapsulates some komplex logic which can not be done natively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(IntegerType())\n",
    "def get_length_udf(str):\n",
    "    return len(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badgesDF.show(truncate=False, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Window definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window().partitionBy('user_id').orderBy('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task:\n",
    "\n",
    "The query bellow is suboptimal. Try to rewrite the query to achive more optimal plan that leads to more efficient execution.\n",
    "\n",
    "Hint:\n",
    "* see the query plan\n",
    "* eliminate the Exchange from the plan\n",
    "* take advantage of the table users, which is bucketed on user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    usersDF\n",
    "    .join(badgesDF, 'user_id')\n",
    "    .withColumn('name_len', get_length_udf('name'))\n",
    "    .withColumn('question_n', row_number().over(w))\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rewrite the query:\n",
    "\n",
    "Hint:\n",
    "* move the UDF before the join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    usersDF\n",
    "    .join(\n",
    "        badgesDF.withColumn('name_len', get_length_udf('name')), \n",
    "        'user_id'\n",
    "    )\n",
    "    .withColumn('question_n', row_number().over(w))\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
