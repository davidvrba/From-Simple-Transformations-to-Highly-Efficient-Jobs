{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, year, round, rand\n",
    ")\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('Bucketing I')\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "\n",
    "project_path = ('/').join(base_path.split('/')[0:-2]) \n",
    "\n",
    "users_input_path = os.path.join(project_path, 'data/users')\n",
    "\n",
    "output_path = os.path.join(project_path, 'output/users-bucketed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "* create a metastore table for users and make the data bucketed by user_id into 10 buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersDF = (\n",
    "    spark\n",
    "    .read\n",
    "    .option('path', users_input_path)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    usersDF\n",
    "    .repartition(10, 'user_id')\n",
    "    .write\n",
    "    .mode('overwrite')\n",
    "    .bucketBy(10, 'user_id')\n",
    "    .sortBy('user_id')\n",
    "    .option('path', output_path)\n",
    "    .saveAsTable('users')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Check the metastore table:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153439"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.table('users').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "| default|    users|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------------------------------------------+-------+\n",
      "|                    col_name|                                                   data_type|comment|\n",
      "+----------------------------+------------------------------------------------------------+-------+\n",
      "|                     user_id|                                                      bigint|   null|\n",
      "|                display_name|                                                      string|   null|\n",
      "|                       about|                                                      string|   null|\n",
      "|                    location|                                                      string|   null|\n",
      "|                   downvotes|                                                      bigint|   null|\n",
      "|                     upvotes|                                                      bigint|   null|\n",
      "|                  reputation|                                                      bigint|   null|\n",
      "|                       views|                                                      bigint|   null|\n",
      "|                            |                                                            |       |\n",
      "|# Detailed Table Information|                                                            |       |\n",
      "|                    Database|                                                     default|       |\n",
      "|                       Table|                                                       users|       |\n",
      "|                       Owner|                                                  david.vrba|       |\n",
      "|                Created Time|                                Fri Mar 15 16:35:13 CET 2019|       |\n",
      "|                 Last Access|                                Thu Jan 01 01:00:00 CET 1970|       |\n",
      "|                  Created By|                                                 Spark 2.4.0|       |\n",
      "|                        Type|                                                    EXTERNAL|       |\n",
      "|                    Provider|                                                     parquet|       |\n",
      "|                 Num Buckets|                                                          10|       |\n",
      "|              Bucket Columns|                                                 [`user_id`]|       |\n",
      "|                Sort Columns|                                                 [`user_id`]|       |\n",
      "|            Table Properties|                          [transient_lastDdlTime=1552664113]|       |\n",
      "|                    Location|file:/Users/david.vrba/From-Simple-Transformations-to-Hig...|       |\n",
      "|               Serde Library|          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe|       |\n",
      "|                 InputFormat|            org.apache.hadoop.mapred.SequenceFileInputFormat|       |\n",
      "|                OutputFormat|   org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat|       |\n",
      "|          Storage Properties|                                    [serialization.format=1]|       |\n",
      "+----------------------------+------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe extended users\").show(truncate=60, n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
